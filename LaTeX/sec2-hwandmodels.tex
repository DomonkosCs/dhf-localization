\subsection{Formulation of the Localization Task}

During localization the main goal is to determine the coordinate transformation between the local coordinate system of the robot,
and a given global frame.
For most of the problems a Global Navigation Satellite System (GNSS), like GPS provides
this information.
In an ideal world the GPS data is precise and reliable, making state estimation based localization algorithms almost unnecessary.
However, it is well known that global positioning by satellites cannot be performed in environments
where the satellite signal is not available, or not strong enough
(for example indoors), and even outdoors, the provided precision is often not sufficient.

To overcome these deficiencies, different state estimation algorithms are used to obtain the pose of the robot in the global frame.
Localization and mapping often goes hand in hand: localization without a map
(or some kind of a representation of the environment on which the global frame is defined),
and map creation without the information about the pose is hardly possible.
If one of them is assumed to be known, the task is much easier: localization, or mapping.
If both are sought after, the Simultaneous Localization and Mapping (SLAM) problem arises,
which is significantly harder than any of them separately.
In the scope of this report, only the localization task is addressed on a given (ground truth) map.

One possible way to achieve localization is the introduction of pose hypotheses.
By this the robot's belief of its pose is a probability distribution, instead of a crisp value~\cite{Thrun2005}.
Knowing the pose exactly is not feasible in a noisy real world environment.
For a localization task, two main hardware components are used: an effector which is responsible for moving the robot,
and an exteroceptive sensor, which is responsible for obtaining information about the surrounding environment (like a LiDAR, a sonar, or often a camera)
\cite{Siegwart2011}.
Both introduce errors and noise to the system which could be dealt with by the application of the probabilistic approach.
In the following, this modelling method is detailed.

Almost every state estimation (e.g. localization) algorithm is based on Bayesian filtering.
It serves as a theoretical foundation for these methods, thus could not be implemented as a
standalone filter.
A Bayes filter consists of two main parts which are iterated over time: prediction and update (see more in \cite{Thrun2005}).
These have the following forms (respectively):
\begin{align}
    \overline{bel}(\mathbf{x}_t) & = \int \underbrace{p(\mathbf{x}_t | \mathbf{x}_{t-1},\mathbf{u}_t)}_{\text{motion model}}bel(\mathbf{x}_{t-1})\mathrm{d}\mathbf{x}_{t-1}, \label{eq:bayes-predict} \\
    bel(\mathbf{x}_t)            & = \eta \underbrace{p(\mathbf{\mathbf{z}}_t | \mathbf{x}_t)}_{\text{meas. model}}\overline{bel}(\mathbf{x}_t),
\end{align}
where
\begin{align}\label{key}
    bel(\mathbf{x}_t)            & = p(\mathbf{x}_t|\mathbf{z}_{1:t},\mathbf{u}_{1:t}),                      \\
    \overline{bel}(\mathbf{x}_t) & = p(\mathbf{x}_t|\mathbf{z}_{1:t-1},\mathbf{u}_{1:t}) \label{eq:predbel}.
\end{align}
The notations are the following: $\mathbf{x}_t$ is the pose (position in 2D, and heading direction) at time $t$,
$\mathbf{u}_t$ is the input (control) vector at $t$, $\eta$ is a normalization constant from  Bayes' theorem,
$\mathbf{z}_t$ is the measurement at $t$, $(\cdot)_{1:t}$ denotes values from time $t = 1$ to $t$,
$bel(\mathbf{x}_t)$ is the belief (also called as posterior), and  $\overline{bel}(\mathbf{x}_t)$ is the predicted belief.

First, $\overline{bel}(\mathbf{x}_t)$ is calculated from the prior $bel(\mathbf{x}_{t-1})$, using the motion model.
This is a prediction, because only the kinematics are incorporated, not the measurements.
Then in the update part the measurement model is considered.
This corrigates (updates) the prediction by incorporating the observations.

However, these probability distributions and integrals cannot be calculated on their own.
Each distinct filter realizations address the solution of the Bayesian recursion differently:
the Kalman Filters use Gaussian distributions and their parametric descriptions to estimate the pose hypotheses,
while particle filters produce a more general numerical solution by representing an arbitrary distribution via particles.
The Daum--Huang filters also use particles, but describes their movement with the help of the Fokker--Planck equation.
These realizations are explained in greater detail in Section \ref{sec:state-estimators}.

\subsection{The Mobile Robot}\label{subsec:robot}
An important preliminary of localization is the introduction of the utilized hardware, the operating environment,
and their models.
In this subsection, the mobile robot itself is discussed, along with its relevant sensors,
followed by the environment representation where the robot has to be localized.

As an agent, the simulated version of ROBOTIS' TurtleBot3\footnote{https://www.robotis.us/turtlebot-3/} is used via Robot Operating System (ROS) and Gazebo.
This two-wheeled platform is widely used for educational and prototyping purposes due to its easy handling
and well developed simulational counterpart. Although it has many useful components, here only the LiDAR and the
differential drive are discussed due to their relevancy in the localization task.
Those and the robot itself can be seen in Figure~\ref{fig:turtlebot3-burger}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{turtlebot3_burger_components.pdf}
    \caption[]{ROBOTIS' TurtleBot3 Burger platform, with a mounted 360$^\circ$ LiDAR on top, and a differential drive (image source: www.robosklep.com).}
    \label{fig:turtlebot3-burger}
\end{figure}

The mounted 2D LiDAR provides range and angle measurements from the environment with $360^\circ$ field of view,
using the triangulation principle (for details, see \cite{Konolige2016}).
This particular model (LDS-01) has an angular resolution of $1^\circ$, detection range of $0.12-3.5$ m,
and accuracy (3$\sigma$) of $\pm$15 mm-s (to be precise, the precision is distance dependent, but this effect is not considered).
Invalid readings indicate out of range measurements. One full measurement is shown in Figure~\ref{fig:lidar-readings}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{lidar-visu.png}
    \caption{Visualized 2D LiDAR measurement of the TurtleBot3 in a house environment, using the Gazebo simulator.}
    \label{fig:lidar-readings}
\end{figure}

The platform has 2 independently-driven wheels, and one free turning wheel, which
makes it eligible to be modelled by differential drive kinematics.
The odometry is conducted by (simulated) rotary encoder readings from the two wheels separately.
Originally, the out-of-the-box ROS-Gazebo model by ROBOTIS did not considered odometry noise,
which had to be manually added in order to efficiently model real world conditions.
Precisely modelling odometry error is a difficult task, and even nowadays is an actively researched topic \cite{Fazekas2021}.
However, this degree of precision is not required here.
Instead, the error model from Subsection 5.2.4. in \cite{Siegwart2011} is utilized.
For a given time interval, denote the distance travelled by the
left and right wheels as $\Delta s_{\text{L}}, \Delta s_{\text{R}}$
respectively. Treat these values as random variables:
\begin{align}
    \Delta s_{\text{L}} & \sim \mathcal{N}(\Delta \hat{s}_{\text{L}},k_{\text{L}}|\Delta \hat{s}_{\text{L}}|), \label{eq:odom-noise1} \\
    \Delta s_{\text{R}} & \sim \mathcal{N}(\Delta \hat{s}_{\text{R}},k_{\text{R}}|\Delta \hat{s}_{\text{R}}|),\label{eq:odom-noise2}
\end{align}
where $\mathcal{N}(\mu,\sigma^{2})$ stands for a normal distribution with mean $\mu$ and variance $\sigma^{2}$, and $k_{\text{L}}$,$k_{\text{R}}$ are error constants.
For the sake of simplicity, $k_{\text{L}} = k_{\text{R}} = \kappa$ is assumed.

By default, Gazebo provides $\Delta \hat{s}_{\text{L}}$ and $\Delta \hat{s}_{\text{R}}$. Based on the drive mechanism and geometry of the robot, the complete odometry is calculated using $\Delta s_{L},\Delta s_{R}$, which is also provided by Gazebo. The level of uncertainty in the odometry is adjusted
by tuning the error constants.


% Instead, both encoder readings are simply corrupted by a random variable each  ($\xi_L$ for the left wheel, $\xi_R$ for the right), obtained as
% \begin{align}
%     \xi_L \sim \mathcal{N}(0,\alpha v_L^{2}), \\
%     \xi_R \sim \mathcal{N}(0,\alpha v_R^{2}),
% \end{align}
% where $\mathcal{N}(\mu,\sigma^{2})$ stands for a normal distribution with mean $\mu$ and variance $\sigma^{2}$,
% $\alpha$ is a scaling parameter, and $v_L, v_R$ are the corresponding velocities. By applying this small modification,
% ROS provides noisy odometry data at each tilmestep, which then can be used to establish the motion model of the robot (see Subsection~\ref{subsec:mot-model}).

\subsection{Environment Representation for Localization}\label{subsec:map}
The environment of the agent is represented by a 2D Occupancy Grid Map (OGM) \cite{Moravec1985}.
This model describes the environment by dividing it to finitely many grid cells, where each grid cell is a random binary variable,
representing that whether it is occupied, or not.
Upon creating the OGM, the occupancy value of each cell (the probability, that it is occupied) is iteratively updated.
To obtain a final map, these values are thresholded, producing values of 1 if the cell is \emph{mostly} occupied,
or 0 if it is \emph{mostly not}.
The top-down view of the TurtleBot3 House by ROBOTIS and the corresponding OGM can be seen in Figure~\ref{fig:tb3-house-ogm}.,
which was obtained by the SLAM GMapping algorithm~\cite{Grisetti2007}.
For a pure localization task, the map is considered as ground truth,
therefore the previously introduced odometry noise was omitted during the mapping process.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{tb3-house-ogm.pdf}
    \caption{Top-down view of the TurtleBot3 House in Gazebo, and its OGM representation (mind the open doors in the building).
        Grey pixels represent unknown area, black pixels are occupied, white pixels are free cells.
        1 pixel (cell) in the OGM has a size of $0.05 \times 0.05$ m. }
    \label{fig:tb3-house-ogm}
\end{figure}
\subsection{The Motion Model}\label{subsec:mot-model}
Now that the hardware and the underlying localization task is introduced, the two main parts of the
Bayesian recursion is going to be described in the following subsections: the motion model, and the measurement model.

The motion model is used to describe the probability distribution of
$p(\mathbf{x}_t | \mathbf{x}_{t-1},\mathbf{u}_t)$ in \eqref{eq:bayes-predict}. \linebreak
Without any external information, the pose of the robot at time $t$ can be estimated based on the previous pose
at time $t-1$, and the control input at time $t$. Naturally, this estimation will be corrupted by tire slippage and drift.
Due to the incremental nature, these errors are integrated over time, thus making the estimation more and more
uncertain.

The motion model describes this transition using the kinematic model of the agent.
Based on the inputs, two distinct probabilistic models can be established, introduced in \cite{Thrun2005}: the velocity motion model,
and the odometry motion model.
The latter is usually more accurate (stated in \cite{Thrun2005}),
furthermore, does not require a timestep explicitly as a parameter.
These beneficial properties make it the ideal choice for this localization task,
thus it is detailed in the following.

If the odometry of the robot is available (i.e. by integrating wheel encoder measurements),
they can be treated as a control input:
\begin{equation}\label{eq:odom-raw-input}
    \hat{\mathbf{u}}_t = (\overline{\mathbf{x}}_t\;\;\overline{\mathbf{x}}_{t-1})^\top =
    \left(\overline{x}_t\;\;\overline{y}_t\;\;\overline{\theta}_t\;\;\overline{x}_{t-1}\;\;\overline{y}_{t-1}\;\;\overline{\theta}_{t-1}\right)^\top.
\end{equation}
The key is the fact that the relative difference between
two consecutive odometry data is a good estimation of the relative difference between
the two consecutive true poses, if the timestep is sufficiently small.

The transition between the state $\mathbf{x}_{t-1}$ and $\mathbf{x}_{t}$ is simplified to a sequence of a rotation, a translation,
and another rotation.
These are indicated in Figure~\ref{fig:odom-mot-model}. with $\delta_{\mathrm{rot1}}$, $\delta_{\mathrm{trans}}$,
and $\delta_{\mathrm{rot2}}$ respectively.
It is important to mention that this separation to rotational and translational components is arbitrary
(introduced by Thrun et al. in~\cite{Thrun2005}); for another approach, see~\cite{Eliazar2004}.
\begin{figure}[htbp]
    \centering
    \includegraphics{odometry-motmodel.pdf}
    \caption{Rotation-translation-rotation transition sequence from state $\mathbf{x}_{t-1}$ to $\mathbf{x}_{t}$.}
    \label{fig:odom-mot-model}
\end{figure}

The odometry control input $\hat{\mathbf{u}}_t$
then transformed to the three transition components as:
\begin{align}
    \delta_{\mathrm{rot1,t}} & =\arctan\!2\left(\overline{y}_{t}-\overline{y}_{t-1}, \overline{x}_{t}-\overline{x}_{t-1}\right)-\overline{\theta}_{t-1}, \\
    \delta_{\text {trans,t}} & =\sqrt{\left(\overline{x}_{t-1}-\overline{x}_{t}\right)^{2}+\left(\overline{y}_{t-1}-\overline{y}_{t}\right)^{2}},        \\
    \delta_{\mathrm{rot2,t}} & =\overline{\theta}_{t}-\overline{\theta}_{t-1}-\delta_{\mathrm{rot} 1},                                                   \\
    \mathbf{u}_t             & = (\delta_{\text{rot1,t}}\;\;\delta_{\text{trans,t}}\;\;\delta_{\text{rot2,t}})^{\top}. \label{eq:odom-delta-input}
\end{align}
To model odometry noise, the inputs are treated as random variables, formulated by
\begin{align}
    \hat{\delta}_{\mathrm{rot1,t}} & = \delta_{\mathrm{rot1,t}} + \xi_{\mathrm{rot1,t}},
    \quad                          & \xi_{\mathrm{rot1,t}} \sim \mathcal{N}(0,\alpha_1\delta_{\mathrm{rot1,t}}^2+\alpha_2\delta_{\mathrm{trans,t}}^2), \label{eq:odom-control-1}                               \\
    \hat{\delta}_{\text {trans,t}} & = \delta_{\mathrm{trans,t}} + \xi_{\mathrm{trans,t}},
    \quad                          & \xi_{\mathrm{trans,t}} \sim \mathcal{N}(0,\alpha_3\delta_{\mathrm{trans,t}}^2+\alpha_4(\delta_{\mathrm{rot1,t}}^2+\delta_{\mathrm{rot2,t}}^2)), \label{eq:odom-control-2} \\
    \hat{\delta}_{\mathrm{rot2,t}} & = \delta_{\mathrm{rot2,t}} + \xi_{\mathrm{rot2,t}},
    \quad                          & \xi_{\mathrm{rot2,t}} \sim \mathcal{N}(0,\alpha_1\delta_{\mathrm{rot2,t}}^2+\alpha_2\delta_{\mathrm{trans,t}}^2), \label{eq:odom-control-3}
\end{align}
where $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ are error parameters.

Then, using the control inputs and the previous state, samples from $p(\mathbf{x}_t | \mathbf{x}_{t-1},\mathbf{u}_t)$
are obtained by
\begin{align}
    x_t      & = x_{t-1} + \hat{\delta}_{\mathrm{trans,t}}\cos(\theta_{t-1} + \hat{\delta}_{\mathrm{rot1,t}}), \label{eq:prop-x} \\
    y_t      & = y_{t-1} + \hat{\delta}_{\mathrm{trans,t}}\sin(\theta_{t-1} + \hat{\delta}_{\mathrm{rot1,t}}), \label{eq:prop-y} \\
    \theta_t & = \theta_{t-1} + \hat{\delta}_{\mathrm{rot1,t}} + \hat{\delta}_{\mathrm{rot2,t}}. \label{eq:prop-theta}
\end{align}
This can be written in the form
\begin{equation}
    \mathbf{x}_t = \phi(\mathbf{x}_{t-1},\mathbf{u}_t,\mathbf{v}_t),
\end{equation}
where
\begin{align}
    \mathbf{x}_t & = (x_t\;\;y_t\;\;\theta_t)^{\top},                                            \\
    \mathbf{v}_t & = (\xi_{\text{rot1,t}}\;\;\xi_{\text{tans,t}}\;\;\xi_{\text{rot2,t}})^{\top}.
\end{align}
Based on \eqref{eq:odom-control-1}-\eqref{eq:odom-control-3}, the covariance matrix of the noise
vector $\mathbf{v}_t$ is
\begin{align}\label{eq:control-covar}
    \mathbf{Q}_t. := & \text{cov}(\mathbf{v}_t,\mathbf{v}_t) \nonumber \\
    =                &
    \text{diag}(\alpha_1\delta_{\mathrm{rot1,t}}^2+\alpha_2\delta_{\mathrm{trans,t}}^2,
    \alpha_3\delta_{\mathrm{trans,t}}^2+\alpha_4(\delta_{\mathrm{rot1,t}}^2+\delta_{\mathrm{rot2,t}}^2),
    \alpha_1\delta_{\mathrm{rot2,t}}^2+\alpha_2\delta_{\mathrm{trans,t}}^2).
\end{align}
For further usages, the Jacobians of $\phi$ are calculated:
\begin{align}
    \nabla\phi_x(\mathbf{x}_{t-1},\mathbf{u}_t) :=
    \left.\frac{\partial\phi}{\partial \mathbf{x}_{t-1}}\right\vert_{\mathbf{x}_{t-1},\mathbf{u}_t}
    =
     & \begin{bmatrix}
        1 & 0 & -\delta_{\text{trans,t}}\sin(\theta_{t-1} + \delta_{\text{rot1,t}}) \\
        0 & 1 & \delta_{\text{trans,t}}\cos(\theta_{t-1} + \delta_{\text{rot1,t}})  \\
        0 & 0 & 0
    \end{bmatrix}, \label{eq:motmodel-jacobi-x} \\
    \nabla\phi_u(\mathbf{x}_{t-1},\mathbf{u}_t) :=
    \left.\frac{\partial\phi}{\partial \mathbf{u}_{t}}\right\vert_{\mathbf{x}_{t-1},\mathbf{u}_t}
    =
     & \begin{bmatrix}
        -\delta_{\text{trans,t}}\sin(\theta_{t-1} + \delta_{\text{rot1,t}}) & \cos(\theta_{t-1} + \delta_{\text{rot1,t}}) & 0 \\
        \delta_{\text{trans,t}}\cos(\theta_{t-1} + \delta_{\text{rot1,t}})  & \sin(\theta_{t-1} + \delta_{\text{rot1,t}}) & 0 \\
        1                                                                   & 0                                           & 1
    \end{bmatrix}. \label{eq:motmodel-jacobi-u}
\end{align}
